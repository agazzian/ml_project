#! /usr/bin/env python

"""
Copyright of the program: Andrea Agazzi, UNIGE
Project started the 19.02.2016

Module containing crossvalidation routines for the main program
"""

import numpy as np

def leave_x_out(nlst,x,nsamples=300,testlst=None):
	"""
	Returns a random sample of a leave-x-out crossvalidation sampling

	nsamples
		Controls the number of crossval-samples generated by the routines.
		Default value = 300
	testlst
		It is possible to sample the elements of the test set out ou a subsample of nlst.
		This is done by specifying the ids of the elements in the subsample in a lists
		flagged as testlst.
		Default value = None (sample from the complete set).
	"""

	if testlst == None:
		testlst = np.arange(len(nlst))
	# enumerate the predictors
	preds = np.arange(len(nlst))
	outlst = []

	# sample nsamples times a new cv sample of size 112-x
	while len(outlst) < nsamples:
		bsample = np.random.choice(testlst,size=x,replace=False)
		outlst.append((np.array([y for y in preds if y not in bsample]),np.array([y for y in bsample])))

	return outlst


def l20o(nlst):
	""" Returns a random sample of size 300 of a leave-20-out crossvalidation sampling"""

	# enumerate the predictors
	preds = range(len(nlst))
	outlst = []

	# sample nsamples times a new cv sample of size 112-x
	while len(outlst) < 300:
		bsample = np.random.choice(preds,size=92,replace=False)
		outlst.append((np.array([x for x in bsample]),np.array([x for x in range(len(nlst)) if x not in bsample])))

	return outlst

def looo(nlst):
	""" Returns a random sample of size "samplesize" of a leave-1-out crossvalidation sampling"""

	# enumerate the predictors
	preds = range(len(nlst))
	outlst = []

	# sample nsamples times a new cv sample of size 112-x
	while len(outlst) < 100:
		bsample = np.random.choice(preds,size=52,replace=False)
		outlst.append((np.array([x for x in bsample]),np.array([x for x in range(len(nlst)) if x not in bsample])))

	return outlst

if __name__ == '__main__':
	print(leave_x_out_2(np.arange(112),5,nsamples=30,testlst=[2,1,3,4,5,12,34,15,23,74,32,12,78,38,40]))


"""
from sklearn import linear_model, decomposition, datasets
from sklearn.pipeline import Pipeline
from sklearn.grid_search import GridSearchCV
"""

"""
logreg = sk.linear_model.LinearDiscriminantAnalysis()

pca = decomposition.PCA()
pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])

digits = datasets.load_digits()
X_digits = digits.data
y_digits = digits.target

###############################################################################
# Plot the PCA spectrum
pca.fit(X_digits)

plt.figure(1, figsize=(4, 3))
plt.clf()
plt.axes([.2, .2, .7, .7])
plt.plot(pca.explained_variance_, linewidth=2)
plt.axis('tight')
plt.xlabel('n_components')
plt.ylabel('explained_variance_')

###############################################################################



# Prediction

n_components = [20, 40, 64]
Cs = np.logspace(-4, 4, 3)
estimator = GridSearchCV(pipe,dict(pca__n_components=n_components,logistic__C=Cs))
estimator.fit(X_digits, y_digits)

plt.axvline(estimator.best_estimator_.named_steps['pca'].n_components,linestyle=':', label='n_components chosen')
plt.legend(prop=dict(size=12))
plt.show()
"""
